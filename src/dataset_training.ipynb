{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Based on the tutorial found at https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
    "#Master\n",
    "\n",
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "\n",
    "# Some basic setup\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "import collections\n",
    "from utils import *\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "from pysilcam.config import PySilcamSettings\n",
    "from pysilcam.process import extract_roi\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sondreab/Desktop/DATA/copepod_lab_petridish/output/model_2020.07.13_14:16:24\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "DIRECTORY = '/home/sondreab/Desktop/DATA/copepod_lab_petridish'\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "runtime = strftime(\"%Y.%m.%d_%H:%M:%S\", gmtime())\n",
    "\n",
    "DATA_DIR = DIRECTORY + '/copepods'\n",
    "VISUALIZE_DIR = DIRECTORY + '/visualize/'+ runtime\n",
    "INFERENCE_DIR = DIRECTORY + '/inference/' + runtime\n",
    "\n",
    "DATASET = 'copepods'\n",
    "\n",
    "OUTPUT_PATH = DIRECTORY + \"/output/\" + \"model_\" + runtime\n",
    "cfg.OUTPUT_DIR = OUTPUT_PATH\n",
    "#os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "print(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in cell is from https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
    "\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "import detectron2.utils.comm as comm\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in cell is from https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "                     \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True)\n",
    "            )\n",
    "        ))\n",
    "        return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file(data, file_name, directory=DIRECTORY):\n",
    "    json_file = os.path.join(directory, file_name + '.json')\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_name, directory=DIRECTORY):\n",
    "    json_file = os.path.join(directory, file_name+'.json')\n",
    "    with open(json_file) as f:\n",
    "        dataset = json.load(f)\n",
    "    for record in dataset:\n",
    "        for obj in record['annotations']:\n",
    "            obj[\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_visualization(dataset, directory=VISUALIZE_DIR):\n",
    "    savepath = VISUALIZE_DIR\n",
    "    print(\"Savepath: {}\".format(savepath))\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    dataset_metadata = MetadataCatalog.get(dataset)\n",
    "    dataset_dicts = read_json_file(dataset, DIRECTORY)\n",
    "    print('Saving dataset '+ dataset)\n",
    "    for image in dataset_dicts:\n",
    "        img = cv2.imread(image[\"file_name\"])\n",
    "        visualizer = Visualizer(img[:, :, ::-1], metadata=dataset_metadata, scale=1)\n",
    "        vis = visualizer.draw_dataset_dict(image)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(savepath, '.'.join(image['file_name'].split('/')[-1].split('.')[:-1]) + '.png'), vis.get_image()[:, :, ::-1])\n",
    "        print('.'.join(image['file_name'].split('/')[-1].split('.')[:-1])+ ' saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coco_dataset_visualization(dataset,dataset_dir = OUTPUT_PATH, directory=VISUALIZE_DIR):\n",
    "    savepath = VISUALIZE_DIR\n",
    "    print(\"Savepath: {}\".format(savepath))\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    dataset_metadata = MetadataCatalog.get(dataset)\n",
    "    dataset_dicts = load_coco(dataset, DIRECTORY)\n",
    "    print('Saving dataset '+ dataset)\n",
    "    for image in dataset_dicts:\n",
    "        img = cv2.imread(image[\"file_name\"])\n",
    "        visualizer = Visualizer(img[:, :, ::-1], metadata=dataset_metadata, scale=1)\n",
    "        vis = visualizer.draw_dataset_dict(image)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(savepath, '.'.join(image['file_name'].split('/')[-1].split('.')[:-1]) + '.png'), vis.get_image()[:, :, ::-1])\n",
    "        print('.'.join(image['file_name'].split('/')[-1].split('.')[:-1])+ ' saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_dataset(directory = DIRECTORY):\n",
    "    thing_classes = ['oil', 'other', 'bubble', 'faremoteecal_pellets', 'copepod', 'diatom_chain', 'oily_gas']\n",
    "    for d in [\"train\", \"val\"]:\n",
    "        DatasetCatalog.register(\"copepod_\" + d, lambda d=d: read_json_file(d, DIRECTORY))\n",
    "        MetadataCatalog.get(\"copepod_\" + d).set(thing_classes=thing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(dataset):\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    #cfg.merge_from_list([\"MODEL.WEIGHTS\", os.path.join(DIRECTORY, \"output/model_final.pth\")])\n",
    "    cfg.DATASETS.TRAIN = (dataset,)\n",
    "    cfg.DATASETS.TEST = ()\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "    cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "    cfg.SOLVER.MAX_ITER = 500    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7  # only has one class (ballon)\n",
    "\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    trainer = DefaultTrainer(cfg) \n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_over_directory(dataset,\n",
    "                             files_dir,\n",
    "                             output_path = OUTPUT_PATH, \n",
    "                             inference_dir = os.path.join(DIRECTORY, INFERENCE_DIR)):\n",
    "    mypath = files_dir\n",
    "    onlyfiles = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "    \n",
    "    cfg.MODEL.WEIGHTS = os.path.join(OUTPUT_PATH, \"model_final.pth\")\n",
    "    \n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model   \n",
    "    \n",
    "    cfg.DATASETS.TEST = (dataset, )\n",
    "\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    savepath = inference_dir\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    \n",
    "    dataset_metadata = MetadataCatalog.get(dataset)\n",
    "    \n",
    "    image_list = onlyfiles\n",
    "    \n",
    "    for image in image_list:\n",
    "        print(os.path.join(mypath,image))\n",
    "        im = cv2.imread(os.path.join(mypath,image))\n",
    "        outputs = predictor(im)\n",
    "        \n",
    "        vis = Visualizer(im[:, :, ::-1],\n",
    "                    metadata=dataset_metadata, \n",
    "                    scale=0.5, \n",
    "                    instance_mode=ColorMode.IMAGE  # remove the colors of unsegmented pixels\n",
    "            )\n",
    "        v = vis.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        cv2.imwrite(os.path.join(savepath, '.'.join(image.split('.')[:-1]) + '.png'), v.get_image()[:, :, ::-1])\n",
    "    print(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [\"train\",  \"val\", \"test\"]\n",
    "thing_classes = ['oil', 'other', 'bubble', 'faecal_pellets', 'copepod', 'diatom_chain', 'oily_gas']\n",
    "for set_ in sets:\n",
    "    dataset = \"my_dataset_\"+set_\n",
    "    file = \"my_dataset_\"+set_\n",
    "    DatasetCatalog.register(dataset, lambda d=file: read_json_file(d, DIRECTORY))\n",
    "    MetadataCatalog.get(dataset).set(thing_classes=thing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "sets = [\"train\",  \"val\", \"test\"]\n",
    "for d in sets:\n",
    "    register_coco_instances(\"my_coco_dataset_\"+d, \n",
    "                            {}, \n",
    "                            \"/home/sondreab/Desktop/DATA/copepod_lab_petridish/my_coco_dataset_\"+d+\".json\", \n",
    "                            \"/home/sondreab/Desktop/DATA/copepod_lab_petridish/copepods\")\n",
    "    \n",
    "register_coco_instances(\"copepod_stats_coco_train\", \n",
    "                            {}, \n",
    "                            \"/home/sondreab/Desktop/DATA/copepod_lab_petridish/copepod_stats_coco_train.json\", \n",
    "                            \"/home/sondreab/Desktop/DATA/copepod_lab_petridish/copepods\")\n",
    "register_coco_instances(\"copepod_stats_coco_test\", \n",
    "                            {}, \n",
    "                            \"/home/sondreab/Desktop/DATA/copepod_lab_petridish/copepod_stats_coco_test.json\", \n",
    "                            \"/home/sondreab/Desktop/DATA/copepod_lab_petridish/copepods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Setup backbone chekcpoint\n",
    "\n",
    "task = \"COCO-SemanticSegmentation\"\n",
    "arch_backbone = \"mask_rcnn_X_101_32x8d_FPN_3x\"\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "#runtime = strftime(\"%Y.%m.%d_%H:%M:%S\", gmtime())\n",
    "\n",
    "OUTPUT_PATH = DIRECTORY + \"/output/\" + \"model_\" + arch_backbone + \"_\" + runtime\n",
    "cfg.OUTPUT_DIR = OUTPUT_PATH\n",
    "#os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "print(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(task + \"/\" + arch_backbone + \".yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"copepod_stats_coco_train\",)\n",
    "cfg.DATASETS.TEST = (\"my_coco_dataset_val\",)\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "#cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(task + \"/\" + arch_backbone + \".yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  \n",
    "cfg.SOLVER.MAX_ITER = 20000  # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "#cfg.SOLVER.CHECKPOINT_PERIOD = 2000\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 5   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7\n",
    "#cfg.MODEL.RETINANET.NUM_CLASSES = 7\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "AP_EVAL_METRICS = {}\n",
    "\n",
    "evaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=OUTPUT_PATH)\n",
    "val_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
    "detectron_metrics = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "AP_EVAL_METRICS[\"detectron\"] = detectron_metrics\n",
    "\n",
    "\n",
    "evaluator = COCOEvaluator(\"my_coco_dataset_test\", cfg, False, output_dir=OUTPUT_PATH)\n",
    "val_loader = build_detection_test_loader(cfg, \"my_coco_dataset_test\")\n",
    "coco_metrics = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "AP_EVAL_METRICS[\"coco\"] = coco_metrics\n",
    "\n",
    "create_json_file(AP_EVAL_METRICS, \"AP_eval_metrics\", OUTPUT_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, \n",
    "              output_path = OUTPUT_PATH, \n",
    "              inference_dir = os.path.join(DIRECTORY, INFERENCE_DIR), \n",
    "              weights = \"model_final.pth\"):\n",
    "    \n",
    "    \n",
    "    cfg.MODEL.WEIGHTS = os.path.join(output_path, weights)\n",
    "    \n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model   \n",
    "    \n",
    "    cfg.DATASETS.TEST = (dataset, )\n",
    "    \n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    savepath = inference_dir\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    \n",
    "    dataset_metadata = MetadataCatalog.get(dataset)\n",
    "    dataset_dicts = read_json_file(dataset, DIRECTORY)\n",
    "    \n",
    "    for image in dataset_dicts:    \n",
    "        im = cv2.imread(image[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        #print(outputs)\n",
    "        #create_json_file(outputs, 'outputs')\n",
    "        \n",
    "        vis = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=dataset_metadata, \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE  # remove the colors of unsegmented pixels\n",
    "        )\n",
    "        v = vis.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        #cv2.imshow('prediction',v.get_image()[:, :, ::-1])\n",
    "        #cv2.waitKey(0)\n",
    "        cv2.imwrite(os.path.join(savepath, '.'.join(image['file_name'].split('/')[-1].split('.')[:-1]) + '.png'), v.get_image()[:, :, ::-1])\n",
    "    print(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(\"my_dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Savepath: /home/sondreab/Desktop/DATA/copepod_lab_petridish/visualize/2020.07.13_14:16:24\n",
      "Saving dataset my_dataset_test\n",
      "D20191125T125406.862891 saved!\n",
      "D20191125T125454.289695 saved!\n",
      "D20191125T125620.539822 saved!\n",
      "D20191125T125624.129360 saved!\n",
      "D20191125T125646.693181 saved!\n",
      "D20191125T125658.361445 saved!\n",
      "D20191125T125700.395411 saved!\n",
      "D20191125T125703.044645 saved!\n",
      "D20191125T125705.342720 saved!\n",
      "D20191125T125755.606911 saved!\n",
      "D20191125T125837.171144 saved!\n",
      "D20191125T125927.111268 saved!\n",
      "D20191125T125944.929349 saved!\n",
      "D20191125T130013.848005 saved!\n",
      "D20191125T130014.430322 saved!\n",
      "D20191125T130021.511633 saved!\n",
      "D20191125T130029.574682 saved!\n",
      "D20191125T130032.396798 saved!\n",
      "D20191125T130033.433702 saved!\n"
     ]
    }
   ],
   "source": [
    "#save_dataset_visualization(\"my_dataset_train\", VISUALIZE_DIR)\n",
    "#save_dataset_visualization(\"my_dataset_val\", VISUALIZE_DIR)\n",
    "save_dataset_visualization(\"my_dataset_test\", VISUALIZE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_visualization(\"my_coco_dataset_train\", VISUALIZE_DIR)\n",
    "save_dataset_visualization(\"my_coco_dataset_val\", VISUALIZE_DIR)\n",
    "save_dataset_visualization(\"my_coco_dataset_test\", VISUALIZE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_over_directory(dataset= \"my_dataset_test\",\n",
    "                         files_dir = os.path.join(DIRECTORY,\"RAWbmp\"),\n",
    "                         output_path = OUTPUT_PATH, \n",
    "                         inference_dir = os.path.join(DIRECTORY, INFERENCE_DIR)+\"_mission_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation] *",
   "language": "python",
   "name": "conda-env-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
