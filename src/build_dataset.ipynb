{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sondreab/anaconda3/envs/segmentation/lib/python3.7/site-packages/tflearn-0.3.2-py3.7.egg/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Based on the tutorial found at https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
    "#Visualize fieldwork 2020-04-20\n",
    "\n",
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "\n",
    "# Some basic setup\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "import collections\n",
    "from utils import *\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from pysilcam.config import PySilcamSettings\n",
    "from pysilcam.process import extract_roi\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = '/home/sondreab/Desktop/DATA/copepod_lab_petridish'\n",
    "DATA_DIR = DIRECTORY + '/copepods'\n",
    "#DATA_DIR = False\n",
    "EXPORT_DIR = DIRECTORY + '/export'\n",
    "STATS_FILE = 'copepods-STATS.csv'\n",
    "\n",
    "\n",
    "VISUALIZE_DIR = DIRECTORY + '/visualize/copepods_2020_07_11'\n",
    "INFERENC_DIR = DIRECTORY + '/inference'\n",
    "\n",
    "DATASET = 'copepod_stats'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pixels(im, bbox):\n",
    "    ''' given a binary image (im) and bounding box (bbox[x_min, y_min, x_max, y_max]), this will return all activated pixel coordinates in x and y\n",
    "\n",
    "    returns:\n",
    "      all_points_x, all_points_y\n",
    "    '''\n",
    "    \n",
    "\n",
    "    roi = im[ bbox[1]:bbox[3], bbox[0]:bbox[2], 0] # bbox[row, column]\n",
    "       \n",
    "    rows = bbox[3] - bbox[1]\n",
    "    coloumns = bbox[2] - bbox[0]\n",
    "    \n",
    "    #print(im.shape)\n",
    "    #print(roi.shape)\n",
    "    #print('({}, {})'.format(rows, coloumns))\n",
    "    #print('iterating')\n",
    "\n",
    "    \n",
    "    contours,_= cv2.findContours(roi, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = np.array(contours)\n",
    "    \"\"\"\n",
    "    print(type(contours))\n",
    "    print(contours[0,:,0,:])\n",
    "    cv2.imshow(\"visualizer\", roi)\n",
    "    cv2.waitKey(0)\n",
    "    \"\"\"\n",
    "    all_points_x, all_points_y = [], []\n",
    "    #print(len(contours.shape))\n",
    "    if len(contours.shape) == 4:\n",
    "        all_points_x, all_points_y = bbox[0] + contours[0,:,0,0], bbox[1] + contours[0,:,0,1]\n",
    "    else:\n",
    "        '''\n",
    "        print(contours[0].shape)\n",
    "        cv2.imshow(\"visualizer\", roi)\n",
    "        cv2.waitKey(0)\n",
    "        '''\n",
    "    \n",
    "        for contour in contours:\n",
    "            if len(contour[:,0,0]) > len(all_points_x):\n",
    "                all_points_x = bbox[0] + contour[:,0,0]\n",
    "                all_points_y = bbox[1] + contour[:,0,1]\n",
    "        #print(all_points_x)\n",
    "        \n",
    "    #print(all_points_x)\n",
    "    #print(all_points_y)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    for r in range(rows):\n",
    "        #print('(r: {})'.format(r))\n",
    "        for c in range(coloumns):\n",
    "            #print('(c: {})'.format(c))\n",
    "            if roi[r,c] == 255:\n",
    "                all_points_x.append(bbox[1] + c)\n",
    "                all_points_y.append(bbox[0] + r)\n",
    "    '''\n",
    "    return all_points_x, all_points_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stats(directory = DIRECTORY):\n",
    "    csv_file = os.path.join(directory+'/proc/', STATS_FILE)\n",
    "    stats = collections.defaultdict(dict)\n",
    "    with open(csv_file, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for rows in reader:\n",
    "            #for key in rows.keys():\n",
    "            stats[rows['export name'].split('-')[0]][rows['particle index']] = rows\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annotation_dictionary(directory = DIRECTORY, size_threshold = 0):\n",
    "    stats = read_stats(directory=directory)\n",
    "\n",
    "    #config_file = os.path.join(directory,'config.ini')\n",
    "    #settings = PySilcamSettings(config_file)\n",
    "\n",
    "    height = 2050\n",
    "    width = 2448\n",
    "\n",
    "    train = []\n",
    "    val = []\n",
    "    dataset = []\n",
    "    print('Image:')\n",
    "    for image, particle in stats.items():\n",
    "        if image == 'not_exported':\n",
    "            continue\n",
    "        print('\\t{}'.format(image))\n",
    "        record = {}\n",
    "        objects = []\n",
    "        record[\"file_name\"] = os.path.join(DATA_DIR, image +'.bmp') #.split('/')[-1]\n",
    "        if not DATA_DIR:\n",
    "            record[\"original_file\"] = ''\n",
    "        else:\n",
    "            record[\"original_file\"] = os.path.join(DATA_DIR, image + '.bmp')\n",
    "        record[\"image_id\"] = image\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        #print('\\tParticle:')\n",
    "        for index, fields in particle.items():\n",
    "            size = float(fields[\"equivalent_diameter\"])\n",
    "            if size < size_threshold:\n",
    "                continue\n",
    "            \n",
    "            #print('\\t{}'.format(index))\n",
    "            \n",
    "            probabilities = np.array([ \n",
    "                              float(fields['probability_oil']),\n",
    "                              float(fields['probability_other']),\n",
    "                              float(fields['probability_bubble']),\n",
    "                              float(fields['probability_faecal_pellets']), \n",
    "                              float(fields['probability_copepod']),\n",
    "                              float(fields['probability_diatom_chain']),\n",
    "                              float(fields['probability_oily_gas'])\n",
    "                              \n",
    "                            ])\n",
    "                \n",
    "            #print(probabilities)\n",
    "            class_probability = np.amax(probabilities)\n",
    "            class_id = np.argmax(probabilities)\n",
    "            \n",
    "            probabilities = [ \n",
    "                              float(fields['probability_oil']),\n",
    "                              float(fields['probability_other']),\n",
    "                              float(fields['probability_bubble']),\n",
    "                              float(fields['probability_faecal_pellets']), \n",
    "                              float(fields['probability_copepod']),\n",
    "                              float(fields['probability_diatom_chain']),\n",
    "                              float(fields['probability_oily_gas'])\n",
    "                            ]\n",
    "            \n",
    "\n",
    "            #if the class is lower than the desired threshold of confidence, skip adding the object\n",
    "            #if (class_probability < 0.7)):                #       settings.Process.threshold)):\n",
    "            #    continue\n",
    "            \n",
    "            minr, minc, maxr, maxc = fields['minr'], fields['minc'], fields['maxr'], fields['maxc']\n",
    "            xmin = int(float(minc))\n",
    "            ymin = int(float(minr))\n",
    "            xmax = int(float(maxc))\n",
    "            ymax = int(float(maxr))\n",
    "            \n",
    "            box_width = xmax - xmin\n",
    "            box_heigth = ymax - ymin\n",
    "            bbox = [xmin, ymin, xmax, ymax]\n",
    "            \n",
    "            im = cv2.imread(os.path.join(EXPORT_DIR, image+'-SEG.bmp'))\n",
    "            if im is None:\n",
    "                print('Image: {} not found, skipping'.format(os.path.join(EXPORT_DIR, image+'-SEG.bmp')))\n",
    "                continue\n",
    "            px, py = extract_pixels(im, bbox)\n",
    "            poly = list(itertools.chain.from_iterable([(x + 0.5, y + 0.5) for x, y in zip(px, py)]))\n",
    "            obj = {\n",
    "                \"bbox\": bbox,\n",
    "                #\"bbox_mode\": BoxMode.XYXY_ABS, JSON cannot seriablize structure.BoxMode, this is set to each object when reading json file.\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": int(class_id),\n",
    "                \"probability\" : class_probability,\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            #print(class_id)\n",
    "            objects.append(obj)\n",
    "        record[\"annotations\"] = objects\n",
    "        dataset.append(record)\n",
    "    #create_json_file(dataset, DATASET, DIRECTORY)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file(data, file_name, directory=EXPORT_DIR):\n",
    "    json_file = os.path.join(directory, file_name + '.json')\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_name, directory=EXPORT_DIR):\n",
    "    json_file = os.path.join(directory, file_name+'.json')\n",
    "    with open(json_file) as f:\n",
    "        dataset = json.load(f)\n",
    "    for record in dataset:\n",
    "        for obj in record['annotations']:\n",
    "            obj[\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_visualization(dataset, directory=VISUALIZE_DIR):\n",
    "    savepath = VISUALIZE_DIR\n",
    "    print(\"Savepath: {}\".format(savepath))\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    copepod_metadata = MetadataCatalog.get(dataset)\n",
    "    dataset_dicts = read_json_file(dataset, DIRECTORY)\n",
    "    print('Saving dataset '+ dataset)\n",
    "    for image in dataset_dicts:\n",
    "        img = cv2.imread(image[\"file_name\"])\n",
    "        if img is None:\n",
    "                print('Image: {} not found, skipping'.format(image[\"file_name\"]))\n",
    "                continue\n",
    "        visualizer = Visualizer(img[:, :, ::-1], metadata=copepod_metadata, scale=1)\n",
    "        vis = visualizer.draw_dataset_dict(image)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(savepath, image['image_id'].split('/')[-1] + '-IMC' + '.png'), vis.get_image()[:, :, ::-1])\n",
    "        print(image['image_id'].split('/')[-1]+ ' saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'copepod_stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = build_annotation_dictionary(size_threshold = 20)\n",
    "\n",
    "create_json_file(data, dataset, DIRECTORY)\n",
    "#read_json_file('annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metadata(name='copepod_stats', thing_classes=['oil', 'other', 'bubble', 'faecal_pellets', 'copepod', 'diatom_chain', 'oily_gas'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing_classes = ['oil', 'other', 'bubble', 'faecal_pellets', 'copepod', 'diatom_chain', 'oily_gas']\n",
    "DatasetCatalog.register(dataset, lambda d=dataset: read_json_file(d, DIRECTORY))\n",
    "MetadataCatalog.get(dataset).set(thing_classes=thing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Savepath: /home/sondreab/Desktop/DATA/copepod_lab_petridish/visualize/copepods_2020_07_11\n",
      "Saving dataset copepod_stats\n",
      "D20191125T125407.768417 saved!\n",
      "D20191125T125406.862891 saved!\n",
      "D20191125T125408.678944 saved!\n",
      "D20191125T125410.525205 saved!\n",
      "D20191125T125411.869320 saved!\n",
      "D20191125T125413.455488 saved!\n",
      "D20191125T125415.039998 saved!\n",
      "D20191125T125416.400461 saved!\n",
      "D20191125T125432.891139 saved!\n",
      "D20191125T125434.018229 saved!\n",
      "D20191125T125453.637554 saved!\n",
      "D20191125T125434.889502 saved!\n",
      "D20191125T125454.289695 saved!\n",
      "D20191125T125455.906403 saved!\n",
      "D20191125T125617.536548 saved!\n",
      "D20191125T125618.680529 saved!\n",
      "D20191125T125619.850613 saved!\n",
      "D20191125T125620.539822 saved!\n",
      "D20191125T125621.870196 saved!\n",
      "D20191125T125622.763127 saved!\n",
      "D20191125T125624.129360 saved!\n",
      "D20191125T125625.281483 saved!\n",
      "D20191125T125634.153291 saved!\n",
      "D20191125T125634.801954 saved!\n",
      "D20191125T125635.450578 saved!\n",
      "D20191125T125635.866842 saved!\n",
      "D20191125T125636.543516 saved!\n",
      "D20191125T125637.160521 saved!\n",
      "D20191125T125644.711687 saved!\n",
      "D20191125T125645.350178 saved!\n",
      "D20191125T125646.693181 saved!\n",
      "D20191125T125647.114662 saved!\n",
      "D20191125T125648.029328 saved!\n",
      "D20191125T125648.478616 saved!\n",
      "D20191125T125652.423412 saved!\n",
      "D20191125T125654.229982 saved!\n",
      "D20191125T125656.969291 saved!\n",
      "D20191125T125658.361445 saved!\n",
      "D20191125T125700.395411 saved!\n",
      "D20191125T125701.048633 saved!\n",
      "D20191125T125701.698599 saved!\n",
      "D20191125T125702.630000 saved!\n",
      "D20191125T125703.044645 saved!\n",
      "D20191125T125703.963334 saved!\n",
      "D20191125T125705.342720 saved!\n",
      "D20191125T125712.655994 saved!\n",
      "D20191125T125713.060316 saved!\n",
      "D20191125T125714.907117 saved!\n",
      "D20191125T125743.949669 saved!\n",
      "D20191125T125745.324058 saved!\n",
      "D20191125T125746.693148 saved!\n",
      "D20191125T125748.061232 saved!\n",
      "D20191125T125749.184613 saved!\n",
      "D20191125T125750.126097 saved!\n",
      "D20191125T125751.063599 saved!\n",
      "D20191125T125752.933600 saved!\n",
      "D20191125T125753.862766 saved!\n",
      "D20191125T125754.733453 saved!\n",
      "D20191125T125755.606911 saved!\n",
      "D20191125T125827.613452 saved!\n",
      "D20191125T125828.259182 saved!\n",
      "D20191125T125828.907082 saved!\n",
      "D20191125T125829.790517 saved!\n",
      "D20191125T125831.418234 saved!\n",
      "D20191125T125832.797221 saved!\n",
      "D20191125T125833.227649 saved!\n",
      "D20191125T125834.582215 saved!\n",
      "D20191125T125839.528300 saved!\n",
      "D20191125T125837.171144 saved!\n",
      "D20191125T125842.295962 saved!\n",
      "D20191125T125846.037451 saved!\n",
      "D20191125T125846.459684 saved!\n",
      "D20191125T125851.615224 saved!\n",
      "D20191125T125855.362121 saved!\n",
      "D20191125T125858.403763 saved!\n",
      "D20191125T125910.273564 saved!\n",
      "D20191125T125910.706289 saved!\n",
      "D20191125T125912.041368 saved!\n",
      "D20191125T125911.604894 saved!\n",
      "D20191125T125913.165354 saved!\n",
      "D20191125T125912.707178 saved!\n",
      "D20191125T125913.611502 saved!\n",
      "D20191125T125915.700055 saved!\n",
      "D20191125T125916.811958 saved!\n",
      "D20191125T125917.933780 saved!\n",
      "D20191125T125919.085158 saved!\n",
      "D20191125T125921.203083 saved!\n",
      "D20191125T125921.651290 saved!\n",
      "D20191125T125923.009277 saved!\n",
      "D20191125T125923.887038 saved!\n",
      "D20191125T125925.248560 saved!\n",
      "D20191125T125927.111268 saved!\n",
      "D20191125T125927.756743 saved!\n",
      "D20191125T125930.277385 saved!\n",
      "D20191125T125941.949314 saved!\n",
      "D20191125T125943.794134 saved!\n",
      "D20191125T125944.929349 saved!\n",
      "D20191125T125946.283731 saved!\n",
      "D20191125T125949.055407 saved!\n",
      "D20191125T125951.177826 saved!\n",
      "D20191125T130002.911423 saved!\n",
      "D20191125T130010.634094 saved!\n",
      "D20191125T130011.292276 saved!\n",
      "D20191125T130011.944204 saved!\n",
      "D20191125T130012.367154 saved!\n",
      "D20191125T130012.788307 saved!\n",
      "D20191125T130013.196780 saved!\n",
      "D20191125T130013.848005 saved!\n",
      "D20191125T130014.430322 saved!\n",
      "D20191125T130015.070029 saved!\n",
      "D20191125T130015.704656 saved!\n",
      "D20191125T130021.046592 saved!\n",
      "D20191125T130021.511633 saved!\n",
      "D20191125T130022.873357 saved!\n",
      "D20191125T130023.517021 saved!\n",
      "D20191125T130024.624629 saved!\n",
      "D20191125T130025.041656 saved!\n",
      "D20191125T130026.865472 saved!\n",
      "D20191125T130028.454714 saved!\n",
      "D20191125T130029.574682 saved!\n",
      "D20191125T130029.992956 saved!\n",
      "D20191125T130030.415661 saved!\n",
      "D20191125T130030.824503 saved!\n",
      "D20191125T130032.396798 saved!\n",
      "D20191125T130032.843406 saved!\n",
      "D20191125T130033.433702 saved!\n"
     ]
    }
   ],
   "source": [
    "save_dataset_visualization(dataset, VISUALIZE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation] *",
   "language": "python",
   "name": "conda-env-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
